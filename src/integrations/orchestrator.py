"""
Security Remediation Orchestrator

Koordiniert ALLE Security-Events in einem Master-Prozess um Race Conditions
und Konflikte zwischen parallelen Fixes zu vermeiden.

Workflow:
1. Sammelt alle Events in einem Zeitfenster (Batch)
2. KI erstellt einen koordinierten Gesamt-Plan
3. User Approval (einmal f√ºr den gesamten Plan)
4. Sequentielle Ausf√ºhrung mit System-Locks
5. Comprehensive Testing und Rollback bei Problemen
"""

import asyncio
import json
import logging
import os
import time
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta

logger = logging.getLogger('shadowops')


@dataclass
class SecurityEventBatch:
    """Batch von Security-Events die zusammen behandelt werden"""
    events: List = field(default_factory=list)
    batch_id: str = ""
    created_at: float = 0.0
    status: str = "collecting"  # collecting, analyzing, awaiting_approval, executing, completed, failed
    status_message_id: Optional[int] = None  # Discord Message ID f√ºr Live-Updates
    status_channel_id: Optional[int] = None  # Discord Channel ID f√ºr Live-Updates

    def __post_init__(self):
        if not self.batch_id:
            self.batch_id = f"batch_{int(time.time())}"
        if not self.created_at:
            self.created_at = time.time()

    @property
    def severity_priority(self) -> int:
        """H√∂chste Severity im Batch (f√ºr Priorisierung)"""
        severity_map = {'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1, 'UNKNOWN': 0}
        return max([severity_map.get(e.severity, 0) for e in self.events], default=0)

    @property
    def sources(self) -> Set[str]:
        """Alle Event-Quellen im Batch"""
        return {e.source for e in self.events}

    def add_event(self, event):
        """F√ºgt Event zum Batch hinzu"""
        self.events.append(event)


@dataclass
class RemediationPlan:
    """Koordinierter Gesamt-Plan f√ºr alle Fixes"""
    batch_id: str
    description: str
    phases: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    estimated_duration_minutes: int = 0
    requires_restart: bool = False
    rollback_plan: str = ""
    ai_model: str = ""
    created_at: float = field(default_factory=time.time)


class RemediationOrchestrator:
    """
    Master Coordinator f√ºr alle Security Remediations

    Verhindert Race Conditions durch:
    - Event Batching (sammelt Events √ºber 10s)
    - Koordinierte KI-Analyse (ALLE Events zusammen)
    - Single Approval Flow
    - Sequentielle Ausf√ºhrung mit System-Locks
    """

    def __init__(self, ai_service, self_healing_coordinator, approval_manager, bot=None, discord_logger=None):
        self.ai_service = ai_service
        self.self_healing = self_healing_coordinator
        self.approval_manager = approval_manager
        self.bot = bot  # Discord Bot f√ºr Approval Messages
        self.discord_logger = discord_logger

        # Event Batching
        self.collection_window_seconds = 10  # Sammelt Events √ºber 10 Sekunden
        self.max_batch_size = 10  # Max 10 Events pro Batch (Server-Schonung)
        self.current_batch: Optional[SecurityEventBatch] = None
        self.batch_lock = asyncio.Lock()
        self.collection_task: Optional[asyncio.Task] = None

        # Execution Lock (nur 1 Remediation zur Zeit!)
        self.execution_lock = asyncio.Lock()
        self.currently_executing: Optional[str] = None

        # Batch Queue
        self.pending_batches: List[SecurityEventBatch] = []
        self.completed_batches: List[SecurityEventBatch] = []

        # NEW: Event History for Learning
        self.event_history: Dict[str, List[Dict]] = {}  # {event_signature: [attempts]}
        self.history_file = 'logs/event_history.json'
        self._load_event_history()

        logger.info("üéØ Remediation Orchestrator initialisiert")
        logger.info(f"   üìä Batching Window: {self.collection_window_seconds}s")
        logger.info(f"   üì¶ Max Batch Size: {self.max_batch_size} Events (Server-Schonung)")
        logger.info("   üîí Sequential Execution Mode: ON")

    def _load_event_history(self):
        """Load event history from disk for learning"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r') as f:
                    self.event_history = json.load(f)
                logger.info(f"üìö Loaded {len(self.event_history)} event type histories")

                # Count total attempts
                total_attempts = sum(len(attempts) for attempts in self.event_history.values())
                if total_attempts > 0:
                    logger.info(f"   üìñ Total historical attempts: {total_attempts}")
            else:
                logger.info("üìö No event history found, starting fresh")
        except Exception as e:
            logger.error(f"‚ùå Error loading event history: {e}")
            self.event_history = {}

    def _save_event_history(self):
        """Save event history to disk for persistence"""
        try:
            os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
            with open(self.history_file, 'w') as f:
                json.dump(self.event_history, f, indent=2, default=str)
            logger.debug("üíæ Event history saved")
        except Exception as e:
            logger.error(f"‚ùå Error saving event history: {e}")

    def _get_status_channel(self):
        """Holt den Status-Channel f√ºr Live-Updates"""
        if not self.bot:
            return None
        # Verwende den Approval-Channel f√ºr Live-Updates
        try:
            approval_channel_id = 1438503737315299351  # auto-remediation-approvals
            channel = self.bot.get_channel(approval_channel_id)
            return channel
        except Exception as e:
            logger.error(f"Fehler beim Holen des Status-Channels: {e}")
        return None

    async def _send_batch_status(self, batch: SecurityEventBatch, status_text: str, color: int = 0xFFAA00):
        """Sendet oder updated Status-Message f√ºr einen Batch"""
        import discord

        channel = self._get_status_channel()
        if not channel:
            logger.warning("‚ö†Ô∏è Status-Channel nicht verf√ºgbar - √ºberspringe Discord-Update")
            return

        try:
            embed = discord.Embed(
                title="üîÑ Koordinierte Remediation l√§uft",
                description=status_text,
                color=color,
                timestamp=datetime.now()
            )
            embed.set_footer(text=f"Batch ID: {batch.batch_id}")

            if batch.status_message_id:
                # Update existing message
                try:
                    message = await channel.fetch_message(batch.status_message_id)
                    await message.edit(embed=embed)
                    logger.debug(f"üìù Discord-Status updated (Message ID: {batch.status_message_id})")
                except:
                    # Message not found, send new one
                    message = await channel.send(embed=embed)
                    batch.status_message_id = message.id
                    batch.status_channel_id = channel.id
                    logger.info(f"üì§ Neue Discord-Status-Message gesendet (ID: {message.id})")
            else:
                # Send new message
                message = await channel.send(embed=embed)
                batch.status_message_id = message.id
                batch.status_channel_id = channel.id
                logger.info(f"üì§ Neue Discord-Status-Message gesendet (ID: {message.id})")

        except Exception as e:
            logger.error(f"Fehler beim Senden der Status-Message: {e}")

    async def submit_event(self, event):
        """
        Event zum Orchestrator hinzuf√ºgen

        Startet automatisch Batch-Collection wenn n√∂tig
        """
        async with self.batch_lock:
            # Erstelle neuen Batch wenn n√∂tig
            if self.current_batch is None:
                self.current_batch = SecurityEventBatch()
                logger.info(f"üì¶ Neuer Event-Batch gestartet: {self.current_batch.batch_id}")

                # Discord Channel Logger: New Batch Started
                if self.discord_logger:
                    self.discord_logger.log_orchestrator(
                        f"üì¶ **Neuer Remediation-Batch gestartet**\n"
                        f"üÜî Batch ID: `{self.current_batch.batch_id}`\n"
                        f"‚è±Ô∏è Collection Window: {self.collection_window_seconds}s",
                        severity="info"
                    )

                # Starte Collection Timer
                self.collection_task = asyncio.create_task(self._close_batch_after_timeout())

                # Sende initiale Discord-Message
                status_text = f"üì¶ **Neuer Remediation-Batch gestartet**\n\n‚è±Ô∏è Sammle Events f√ºr {self.collection_window_seconds} Sekunden..."
                await self._send_batch_status(self.current_batch, status_text, 0x3498DB)

            # F√ºge Event zum aktuellen Batch hinzu
            self.current_batch.add_event(event)
            logger.info(f"   ‚ûï Event hinzugef√ºgt: {event.source} ({event.severity})")
            logger.info(f"   üìä Batch Status: {len(self.current_batch.events)}/{self.max_batch_size} Events")

            # Check if batch size limit reached
            if len(self.current_batch.events) >= self.max_batch_size:
                logger.info(f"‚ö†Ô∏è Batch Limit erreicht ({self.max_batch_size} Events) - Schlie√üe Batch sofort")
                # Cancel collection timer and close batch immediately
                if self.collection_task:
                    self.collection_task.cancel()
                await self._close_batch_immediately()
                return

            # Update Discord-Message mit neuem Event
            event_list = "\n".join([f"‚Ä¢ **{e.source.upper()}**: {e.severity}" for e in self.current_batch.events])
            elapsed = int(time.time() - self.current_batch.created_at)
            remaining = max(0, self.collection_window_seconds - elapsed)
            status_text = f"üì¶ **Sammle Security-Events**\n\n{event_list}\n\n‚è±Ô∏è Verbleibend: **{remaining}s** | Events: **{len(self.current_batch.events)}/{self.max_batch_size}**"
            await self._send_batch_status(self.current_batch, status_text, 0x3498DB)

    async def _close_batch_after_timeout(self):
        """Schlie√üt Batch nach Collection Window mit Live-Countdown-Updates"""
        update_interval = 2  # Update Discord alle 2 Sekunden
        elapsed = 0

        batch = self.current_batch  # Referenz speichern

        while elapsed < self.collection_window_seconds:
            await asyncio.sleep(update_interval)
            elapsed += update_interval

            # Update Discord mit Countdown
            async with self.batch_lock:
                if self.current_batch == batch and len(batch.events) > 0:
                    remaining = max(0, self.collection_window_seconds - elapsed)
                    event_list = "\n".join([f"‚Ä¢ **{e.source.upper()}**: {e.severity}" for e in batch.events])

                    # Progress bar
                    progress = min(100, int((elapsed / self.collection_window_seconds) * 100))
                    bar_length = 20
                    filled = int((progress / 100) * bar_length)
                    bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

                    status_text = f"üì¶ **Sammle Security-Events**\n\n{event_list}\n\n‚è±Ô∏è **{remaining}s** verbleibend | Events: **{len(batch.events)}**\n\n{bar} {progress}%"
                    await self._send_batch_status(batch, status_text, 0x3498DB)

        async with self.batch_lock:
            if self.current_batch and len(self.current_batch.events) > 0:
                logger.info(f"‚è∞ Batch-Collection abgelaufen ({self.collection_window_seconds}s)")
                logger.info(f"   üì¶ Batch {self.current_batch.batch_id}: {len(self.current_batch.events)} Events")
                logger.info(f"   üîç Quellen: {', '.join(self.current_batch.sources)}")

                # Final Discord Update
                event_list = "\n".join([f"‚Ä¢ **{e.source.upper()}**: {e.severity}" for e in self.current_batch.events])
                status_text = f"‚úÖ **Batch geschlossen**\n\n{event_list}\n\nüìä Total: **{len(self.current_batch.events)} Events**\nüîç Quellen: {', '.join(self.current_batch.sources)}\n\nüß† Starte KI-Analyse..."
                await self._send_batch_status(self.current_batch, status_text, 0xF39C12)

                # Batch zur Verarbeitung verschieben
                self.current_batch.status = "analyzing"
                self.pending_batches.append(self.current_batch)
                self.current_batch = None

                # Starte Verarbeitung
                asyncio.create_task(self._process_next_batch())

    async def _close_batch_immediately(self):
        """Schlie√üt Batch sofort wenn Max-Size erreicht ist (Server-Schonung)"""
        if self.current_batch and len(self.current_batch.events) > 0:
            logger.info(f"üì¶ Batch {self.current_batch.batch_id}: {len(self.current_batch.events)} Events (LIMIT)")
            logger.info(f"   üîç Quellen: {', '.join(self.current_batch.sources)}")

            # Final Discord Update
            event_list = "\n".join([f"‚Ä¢ **{e.source.upper()}**: {e.severity}" for e in self.current_batch.events])
            status_text = f"‚ö†Ô∏è **Batch Limit erreicht** ({self.max_batch_size} Events)\n\n{event_list}\n\nüìä Total: **{len(self.current_batch.events)} Events**\nüîç Quellen: {', '.join(self.current_batch.sources)}\n\nüß† Starte KI-Analyse..."
            await self._send_batch_status(self.current_batch, status_text, 0xF39C12)

            # Batch zur Verarbeitung verschieben
            self.current_batch.status = "analyzing"
            self.pending_batches.append(self.current_batch)
            self.current_batch = None

            # Starte Verarbeitung
            asyncio.create_task(self._process_next_batch())

    async def _process_next_batch(self):
        """Verarbeitet n√§chsten Batch (mit Execution Lock!)"""

        # Warte auf Execution Lock (nur 1 Remediation gleichzeitig!)
        if self.execution_lock.locked():
            logger.info("‚è≥ Execution Lock aktiv - warte auf Abschluss der laufenden Remediation...")
            return

        async with self.execution_lock:
            if not self.pending_batches:
                return

            # Hole Batch mit h√∂chster Priorit√§t
            batch = max(self.pending_batches, key=lambda b: b.severity_priority)
            self.pending_batches.remove(batch)
            self.currently_executing = batch.batch_id

            logger.info(f"üöÄ Starte koordinierte Remediation f√ºr Batch {batch.batch_id}")
            logger.info(f"   üìä {len(batch.events)} Events aus {len(batch.sources)} Quellen")

            try:
                # Phase 1: KI erstellt koordinierten Gesamt-Plan
                logger.info("üß† Phase 1: KI-Analyse aller Events...")
                plan = await self._create_coordinated_plan(batch)

                if not plan:
                    logger.error(f"‚ùå KI konnte keinen Plan erstellen f√ºr Batch {batch.batch_id}")
                    batch.status = "failed"
                    self.completed_batches.append(batch)
                    return

                logger.info(f"‚úÖ Koordinierter Plan erstellt:")
                logger.info(f"   üìù {len(plan.phases)} Phasen")
                logger.info(f"   ‚è±Ô∏è  Gesch√§tzte Dauer: {plan.estimated_duration_minutes} Minuten")
                logger.info(f"   üéØ Confidence: {plan.confidence:.0%}")

                # Phase 2: User Approval (einmal f√ºr ALLES)
                logger.info("üë§ Phase 2: Warte auf User-Approval...")
                approved = await self._request_approval(batch, plan)

                if not approved:
                    logger.warning(f"‚ùå User hat Batch {batch.batch_id} abgelehnt")
                    batch.status = "rejected"
                    self.completed_batches.append(batch)
                    return

                # Phase 3: Sequentielle Ausf√ºhrung
                logger.info("‚öôÔ∏è Phase 3: Sequentielle Ausf√ºhrung...")
                batch.status = "executing"
                success = await self._execute_plan(batch, plan)

                if success:
                    logger.info(f"‚úÖ Batch {batch.batch_id} erfolgreich abgeschlossen!")
                    batch.status = "completed"
                else:
                    logger.error(f"‚ùå Batch {batch.batch_id} fehlgeschlagen")
                    batch.status = "failed"

                self.completed_batches.append(batch)

            except Exception as e:
                logger.error(f"‚ùå Orchestrator Error f√ºr Batch {batch.batch_id}: {e}", exc_info=True)
                batch.status = "failed"
                self.completed_batches.append(batch)

            finally:
                self.currently_executing = None

                # Verarbeite n√§chsten Batch falls vorhanden
                if self.pending_batches:
                    asyncio.create_task(self._process_next_batch())

    async def _create_coordinated_plan(self, batch: SecurityEventBatch) -> Optional[RemediationPlan]:
        """
        KI erstellt koordinierten Gesamt-Plan f√ºr ALLE Events zusammen

        Wichtig: Die KI analysiert alle Events zusammen und erkennt:
        - Abh√§ngigkeiten zwischen Fixes
        - Optimale Reihenfolge
        - Gemeinsame Schritte (z.B. ein Backup f√ºr alle)
        """

        # Sende initiale Discord-Message: KI-Analyse startet
        status_text = "üß† **KI-Analyse startet**\n\nLlama3.1 analysiert alle Events und erstellt koordinierten Plan...\n\n‚è≥ Dies kann 2-3 Minuten dauern"
        await self._send_batch_status(batch, status_text, 0xF39C12)  # Orange

        # Build comprehensive context with ALL events
        context = {
            'batch_id': batch.batch_id,
            'events': [e.to_dict() for e in batch.events],
            'event_count': len(batch.events),
            'sources': list(batch.sources),
            'highest_severity': max([e.severity for e in batch.events], key=lambda s: {'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}.get(s, 0)),
            'is_coordinated_planning': True  # Flag for JSON parser
        }

        # Create streaming state for live Discord updates
        streaming_state = {
            'token_count': 0,
            'last_snippet': '',
            'batch': batch,
            'start_time': time.time()
        }
        context['streaming_state'] = streaming_state

        # Special prompt for coordinated planning
        prompt = self._build_coordinated_planning_prompt(context)

        # Use AI to create coordinated plan
        logger.info("üß† Rufe KI f√ºr koordinierte Planung auf...")

        # Start background task for live Discord updates w√§hrend Streaming
        update_task = asyncio.create_task(self._stream_ai_progress_to_discord(streaming_state))

        try:
            # Use generate_coordinated_plan with coordinated planning context
            result = await self.ai_service.generate_coordinated_plan(prompt, context)

            # Stop streaming updates
            streaming_state['done'] = True
            await update_task  # Wait for final update

            if not result:
                logger.error("‚ùå KI konnte keinen koordinierten Plan erstellen")
                status_text = "‚ùå **KI-Analyse fehlgeschlagen**\n\nKonnte keinen koordinierten Plan erstellen"
                await self._send_batch_status(batch, status_text, 0xE74C3C)  # Red
                return None

            # Parse AI response into RemediationPlan
            plan = RemediationPlan(
                batch_id=batch.batch_id,
                description=result.get('description', 'Koordinierte Remediation'),
                phases=result.get('phases', []),
                confidence=result.get('confidence', 0.0),
                estimated_duration_minutes=result.get('estimated_duration_minutes', 30),
                requires_restart=result.get('requires_restart', False),
                rollback_plan=result.get('rollback_plan', 'Automatisches Rollback via Backups'),
                ai_model=result.get('ai_model', 'unknown')
            )

            # Sende finale Discord-Message: Plan erstellt
            phase_names = "\n".join([f"‚Ä¢ **Phase {i+1}**: {p['name']}" for i, p in enumerate(plan.phases)])
            status_text = f"‚úÖ **Plan erstellt**\n\n{phase_names}\n\n‚è±Ô∏è Gesch√§tzte Dauer: **{plan.estimated_duration_minutes}min**\nüéØ Confidence: **{plan.confidence:.0%}**"
            await self._send_batch_status(batch, status_text, 0x2ECC71)  # Green

            logger.info(f"‚úÖ Koordinierter Plan erstellt: {len(plan.phases)} Phasen, {plan.confidence:.0%} Confidence")
            return plan

        except Exception as e:
            # Stop streaming updates on error
            streaming_state['done'] = True
            try:
                await update_task
            except:
                pass

            logger.error(f"‚ùå Fehler bei koordinierter Planung: {e}", exc_info=True)
            status_text = f"‚ùå **KI-Analyse fehlgeschlagen**\n\nFehler: {str(e)}"
            await self._send_batch_status(batch, status_text, 0xE74C3C)  # Red
            return None

    async def _stream_ai_progress_to_discord(self, streaming_state: Dict):
        """
        Monitored streaming_state und sendet Live-Updates w√§hrend KI-Analyse
        """
        batch = streaming_state['batch']
        update_interval = 5  # Update Discord alle 5 Sekunden
        expected_tokens = 400  # Llama3.1 generiert ~400 tokens f√ºr einen Plan

        last_token_count = 0

        while not streaming_state.get('done', False):
            await asyncio.sleep(update_interval)

            token_count = streaming_state.get('token_count', 0)
            last_snippet = streaming_state.get('last_snippet', '')
            elapsed = int(time.time() - streaming_state['start_time'])

            # Nur updaten wenn neue Tokens generiert wurden
            if token_count > last_token_count:
                last_token_count = token_count

                # Progress bar basierend auf Token-Count
                progress = min(100, int((token_count / expected_tokens) * 100))
                bar_length = 20
                filled = int((progress / 100) * bar_length)
                bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

                # Format snippet f√ºr Discord (max 100 chars)
                snippet_preview = last_snippet[:100] + "..." if len(last_snippet) > 100 else last_snippet

                # Gesch√§tzte Restzeit (basierend auf bisheriger Speed)
                if token_count > 0 and elapsed > 0:
                    tokens_per_sec = token_count / elapsed
                    remaining_tokens = max(0, expected_tokens - token_count)
                    eta_seconds = int(remaining_tokens / tokens_per_sec) if tokens_per_sec > 0 else 0
                    eta_text = f"‚è±Ô∏è ETA: ~{eta_seconds}s"
                else:
                    eta_text = "‚è±Ô∏è ETA: Berechne..."

                # Phase detection aus snippet
                phase_info = ""
                if "Phase 1" in last_snippet or "Backup" in last_snippet:
                    phase_info = "üîç Analysiere: **Phase 1 (Backup)**"
                elif "Phase 2" in last_snippet or "Docker" in last_snippet or "Update" in last_snippet:
                    phase_info = "üîç Analysiere: **Phase 2 (Updates)**"
                elif "Phase 3" in last_snippet or "trivy" in last_snippet.lower() or "Remediation" in last_snippet:
                    phase_info = "üîç Analysiere: **Phase 3 (Remediation)**"
                elif token_count > 50:
                    phase_info = "üîç Analysiere: **Sicherheitsplan**"

                status_text = f"üß† **KI-Analyse l√§uft**\n\n{phase_info}\n\nüìä Tokens: **{token_count}** / ~{expected_tokens}\n‚ö° Zeit: **{elapsed}s** | {eta_text}\n\n{bar} {progress}%"

                # F√ºge snippet hinzu falls vorhanden
                if snippet_preview:
                    status_text += f"\n\nüí¨ *\"{snippet_preview}\"*"

                await self._send_batch_status(batch, status_text, 0xF39C12)  # Orange

        # Finale Message falls noch nicht von _create_coordinated_plan() gesendet
        # (kann passieren wenn done=True gesetzt wird bevor letzte Update)

    def _build_coordinated_planning_prompt(self, context: Dict) -> str:
        """Baut Prompt f√ºr koordinierte Planung mit Infrastructure Context"""

        prompt_parts = []

        # ADD: Context Manager Integration for Infrastructure Knowledge
        if self.ai_service and hasattr(self.ai_service, 'context_manager') and self.ai_service.context_manager:
            prompt_parts.append("# INFRASTRUCTURE & PROJECT KNOWLEDGE BASE")
            prompt_parts.append("Du hast Zugriff auf detaillierte Informationen √ºber die Server-Infrastruktur und laufende Projekte.")
            prompt_parts.append("Nutze diesen Kontext f√ºr informierte, sichere Entscheidungen.\n")

            # Get relevant context for all events in batch
            for event in context['events']:
                relevant_context = self.ai_service.context_manager.get_relevant_context(
                    event['source'],
                    event.get('event_type', 'unknown')
                )
                if relevant_context:
                    prompt_parts.append(relevant_context)
                    break  # Only add context once (same for all events in batch)

            prompt_parts.append("\n" + "="*80 + "\n")

        # Main coordination prompt
        prompt_parts.append(f"""# Koordinierte Security Remediation

Du bist ein Security-Engineer der einen KOORDINIERTEN Gesamt-Plan erstellt.

## Wichtig:
- Analysiere ALLE {context['event_count']} Events ZUSAMMEN
- Nutze den INFRASTRUCTURE & PROJECT KNOWLEDGE BASE Kontext oben
- Erkenne Abh√§ngigkeiten zwischen Projekten und Services
- Erstelle EINE sequentielle Ausf√ºhrungs-Pipeline
- Vermeide Race Conditions und Breaking Changes
- Ber√ºcksichtige laufende Services (docker-compose.yml, Versionen)

## Events im Batch:
""")

        for i, event in enumerate(context['events'], 1):
            prompt_parts.append(f"\n### Event {i}: {event['source']} ({event['severity']})\n")
            prompt_parts.append(f"```\n{event.get('details', 'N/A')}\n```\n")

        prompt_parts.append("""

## Aufgabe:
Erstelle einen koordinierten Plan mit Phasen die NACHEINANDER ausgef√ºhrt werden.

**WICHTIG: Alle Texte M√úSSEN auf DEUTSCH sein!**

Ausgabe als JSON:
{
  "description": "Kurze Beschreibung des Gesamt-Plans (DEUTSCH)",
  "confidence": 0.XX,
  "estimated_duration_minutes": XX,
  "requires_restart": true/false,
  "phases": [
    {
      "name": "Phase 1: Backup",
      "description": "System-Backup erstellen",
      "steps": ["Schritt 1", "Schritt 2"],
      "estimated_minutes": 5
    },
    {
      "name": "Phase 2: Docker Updates",
      "description": "CVEs in Docker Images beheben",
      "steps": ["Update packages", "Rebuild images", "Test"],
      "estimated_minutes": 15
    }
  ],
  "rollback_plan": "Beschreibung wie Rollback funktioniert (DEUTSCH)"
}
""")

        return "\n".join(prompt_parts)

    async def _request_approval(self, batch: SecurityEventBatch, plan: RemediationPlan) -> bool:
        """
        Fordert User-Approval f√ºr den gesamten koordinierten Plan an

        Zeigt ein sch√∂nes Discord Embed mit:
        - Zusammenfassung aller Events
        - Alle Phasen des Plans
        - Gesch√§tzte Dauer
        - Risiko-Level
        - Approve/Reject Buttons
        """
        import discord

        logger.info(f"üë§ Fordere Approval an f√ºr Batch {batch.batch_id}")

        # Build Discord Embed
        embed = discord.Embed(
            title="üéØ Koordinierter Remediation-Plan",
            description=f"**{plan.description}**\n\nDieser Plan behandelt **{len(batch.events)} Security-Events** koordiniert und sequentiell.",
            color=discord.Color.gold(),
            timestamp=datetime.now()
        )

        # Events Summary
        sources_summary = {}
        for event in batch.events:
            source = event.source
            if source not in sources_summary:
                sources_summary[source] = {'count': 0, 'severity': event.severity}
            sources_summary[source]['count'] += 1

        events_text = "\n".join([
            f"**{source.upper()}:** {info['count']} Event(s) ({info['severity']})"
            for source, info in sources_summary.items()
        ])

        embed.add_field(
            name="üì¶ Events im Batch",
            value=events_text,
            inline=False
        )

        # Execution Plan (Phasen) - Discord Field limit: 1024 characters
        phases_text = ""
        total_minutes = 0
        max_desc_length = 120  # Max chars per phase description

        for i, phase in enumerate(plan.phases[:5], 1):  # Max 5 Phasen anzeigen
            name = phase.get('name', f'Phase {i}')
            desc = phase.get('description', 'N/A')
            minutes = phase.get('estimated_minutes', 5)
            total_minutes += minutes

            # Truncate description if too long
            if len(desc) > max_desc_length:
                desc = desc[:max_desc_length] + "..."

            phase_text = f"**{i}. {name}** (~{minutes}min)\n{desc}\n\n"

            # Check if adding this phase would exceed Discord's 1024 char limit
            if len(phases_text) + len(phase_text) > 1020:  # Leave some margin
                phases_text += f"_...und {len(plan.phases) - (i-1)} weitere Phasen_\n"
                break

            phases_text += phase_text

        if len(plan.phases) > 5 and len(phases_text) < 1020:
            phases_text += f"_...und {len(plan.phases) - 5} weitere Phasen_\n"

        # Ensure we never exceed 1024 characters (Discord limit)
        if len(phases_text) > 1024:
            phases_text = phases_text[:1020] + "..."

        embed.add_field(
            name="‚öôÔ∏è Ausf√ºhrungs-Plan",
            value=phases_text or "Keine Phasen definiert",
            inline=False
        )

        # Metadata
        confidence_color = "üü¢" if plan.confidence >= 0.8 else "üü°" if plan.confidence >= 0.6 else "üî¥"

        embed.add_field(
            name="üìä Plan-Details",
            value=f"**Confidence:** {confidence_color} {plan.confidence:.0%}\n"
                  f"**Gesch√§tzte Dauer:** ‚è±Ô∏è ~{total_minutes} Minuten\n"
                  f"**Neustart erforderlich:** {'‚úÖ Ja' if plan.requires_restart else '‚ùå Nein'}\n"
                  f"**KI-Modell:** {plan.ai_model}",
            inline=False
        )

        # Rollback Info
        if plan.rollback_plan:
            embed.add_field(
                name="üîÑ Rollback-Strategie",
                value=plan.rollback_plan[:200] + ("..." if len(plan.rollback_plan) > 200 else ""),
                inline=False
            )

        embed.set_footer(text=f"Batch ID: {batch.batch_id} | Orchestrator v1.0")

        # Send to approval channel with buttons
        try:
            if not self.bot:
                logger.warning("‚ö†Ô∏è Kein Bot verf√ºgbar f√ºr Approval - Auto-Approve")
                return True

            # Get approval channel
            approval_channel_id = 1438503737315299351  # auto-remediation-approvals
            channel = self.bot.get_channel(approval_channel_id)

            if not channel:
                logger.error(f"‚ùå Approval Channel {approval_channel_id} nicht gefunden")
                return False

            # Create approval buttons
            import discord

            class ApprovalView(discord.ui.View):
                def __init__(self, orchestrator, batch_id):
                    super().__init__(timeout=1800)  # 30 minutes
                    self.orchestrator = orchestrator
                    self.batch_id = batch_id
                    self.approved = None

                @discord.ui.button(label="‚úÖ Approve & Execute", style=discord.ButtonStyle.green, custom_id="approve")
                async def approve_button(self, interaction: discord.Interaction, button: discord.ui.Button):
                    await interaction.response.send_message(
                        f"‚úÖ **Plan approved!** Starte koordinierte Remediation...",
                        ephemeral=True
                    )
                    self.approved = True
                    self.stop()

                @discord.ui.button(label="‚ùå Reject", style=discord.ButtonStyle.red, custom_id="reject")
                async def reject_button(self, interaction: discord.Interaction, button: discord.ui.Button):
                    await interaction.response.send_message(
                        f"‚ùå **Plan abgelehnt.** Remediation wird nicht ausgef√ºhrt.",
                        ephemeral=True
                    )
                    self.approved = False
                    self.stop()

                @discord.ui.button(label="üìã Details anzeigen", style=discord.ButtonStyle.gray, custom_id="details")
                async def details_button(self, interaction: discord.Interaction, button: discord.ui.Button):
                    # Build detailed view
                    details_text = f"**Batch {self.batch_id} - Detaillierte Phasen:**\n\n"

                    # Get plan from orchestrator
                    # For now, just acknowledge
                    await interaction.response.send_message(
                        f"üìã Detaillierte Phasen-Informationen f√ºr Batch `{self.batch_id}`\n\n"
                        f"Siehe Embed oben f√ºr vollst√§ndige Details.",
                        ephemeral=True
                    )

            # Create view instance
            view = ApprovalView(self, batch.batch_id)

            # Send message with embed and buttons
            approval_message = await channel.send(embed=embed, view=view)
            logger.info(f"üì¨ Approval-Request gesendet an Channel {channel.name}")

            # Wait for user interaction
            logger.info(f"‚è≥ Warte auf User-Approval (Timeout: 30min)...")
            await view.wait()

            # Update message to show result
            if view.approved is True:
                # Update embed color to green
                embed.color = discord.Color.green()
                embed.title = "‚úÖ Plan Approved - Wird ausgef√ºhrt"
                await approval_message.edit(embed=embed, view=None)
                logger.info(f"‚úÖ Batch {batch.batch_id} wurde approved")
                return True

            elif view.approved is False:
                # Update embed color to red
                embed.color = discord.Color.red()
                embed.title = "‚ùå Plan Rejected"
                await approval_message.edit(embed=embed, view=None)
                logger.warning(f"‚ùå Batch {batch.batch_id} wurde rejected")
                return False

            else:
                # Timeout
                embed.color = discord.Color.dark_gray()
                embed.title = "‚è∞ Approval Timeout - Plan verworfen"
                await approval_message.edit(embed=embed, view=None)
                logger.warning(f"‚è∞ Batch {batch.batch_id} - Approval Timeout")
                return False

        except Exception as e:
            logger.error(f"‚ùå Fehler bei Approval-Request: {e}", exc_info=True)
            return False

    async def _execute_plan(self, batch: SecurityEventBatch, plan: RemediationPlan) -> bool:
        """
        F√ºhrt Plan sequentiell Phase f√ºr Phase aus

        Workflow:
        1. Erstelle System-Backup
        2. F√ºhre jede Phase nacheinander aus
        3. Teste nach jeder Phase
        4. Bei Fehler: Rollback und Stop
        5. Sende Discord-Updates w√§hrend Ausf√ºhrung

        MULTI-PROJECT MODE:
        - Erkennt wenn mehrere Projekte betroffen sind
        - F√ºhrt Projekte sequentiell aus (eins nach dem anderen)
        - F√ºr jedes Projekt: Backup ‚Üí Fix ‚Üí Verify Scan ‚Üí Check Success
        - Nur wenn Projekt erfolgreich: Fahre mit n√§chstem fort
        - Bei Fehler: Rollback und Retry mit AI Learning
        """
        import discord
        from datetime import datetime

        logger.info(f"‚öôÔ∏è Starte sequentielle Ausf√ºhrung von {len(plan.phases)} Phasen")

        # Check for multi-project batch
        projects_map = self._group_events_by_project(batch.events)
        multi_project_mode = len(projects_map) > 1

        if multi_project_mode:
            logger.info(f"üê≥ MULTI-PROJECT MODE erkannt: {len(projects_map)} Projekte betroffen")
            for project_path, project_events in projects_map.items():
                project_name = project_path.split('/')[-1]
                logger.info(f"   üìÇ {project_name}: {len(project_events)} Events")

        # Discord Channel Logger: Execution Start
        if self.discord_logger:
            if multi_project_mode:
                project_list = "\n".join([f"   üìÇ {p.split('/')[-1]}" for p in projects_map.keys()])
                self.discord_logger.log_orchestrator(
                    f"‚öôÔ∏è **MULTI-PROJECT Execution gestartet**\n"
                    f"üÜî Batch: `{batch.batch_id}`\n"
                    f"üê≥ Projekte: **{len(projects_map)}**\n{project_list}\n"
                    f"üìã Phasen: **{len(plan.phases)}**\n"
                    f"‚ö†Ô∏è Sequentielle Verarbeitung: Eins nach dem anderen",
                    severity="info"
                )
            else:
                self.discord_logger.log_orchestrator(
                    f"‚öôÔ∏è **Execution gestartet**\n"
                    f"üÜî Batch: `{batch.batch_id}`\n"
                    f"üìã Phasen: **{len(plan.phases)}**\n"
                    f"‚è±Ô∏è Est. Duration: {plan.estimated_duration_minutes}min",
                    severity="info"
                )

        # MULTI-PROJECT MODE: Process projects sequentially
        if multi_project_mode:
            return await self._execute_multi_project_plan(batch, plan, projects_map)

        # SINGLE PROJECT MODE: Original execution flow

        # Track execution start time for duration calculation
        self._execution_start_time = datetime.now()

        # Get execution channel for live updates
        execution_channel = None
        if self.bot:
            try:
                # Send to remediation-alerts channel for live updates
                channel_id = 1438503736220586164  # auto-remediation-alerts
                execution_channel = self.bot.get_channel(channel_id)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Konnte Execution-Channel nicht laden: {e}")

        # Create execution status embed
        exec_embed = None
        exec_message = None

        if execution_channel:
            exec_embed = discord.Embed(
                title="‚öôÔ∏è Koordinierte Remediation l√§uft",
                description=f"**Batch {batch.batch_id}**\n{plan.description}",
                color=discord.Color.blue(),
                timestamp=datetime.now()
            )
            exec_embed.add_field(
                name="üìä Status",
                value="üîÑ Starte Ausf√ºhrung...",
                inline=False
            )
            exec_message = await execution_channel.send(embed=exec_embed)

        # Track execution results
        executed_phases = []
        backup_created = False
        backup_path = None

        try:
            # Phase 0: Create system backup
            logger.info("üíæ Phase 0: Erstelle System-Backup...")
            if exec_message:
                exec_embed.set_field_at(
                    0,
                    name="üìä Status",
                    value="üíæ Phase 0/0: System-Backup wird erstellt...",
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

            # Create backup using BackupManager from self_healing
            backup_manager = self.self_healing.backup_manager
            backup_metadata = []

            # Collect files to backup based on events
            files_to_backup = set()
            for event in batch.events:
                if event.source == 'trivy':
                    # Backup Docker-related files
                    files_to_backup.add('/home/cmdshadow/shadowops-bot/package.json')
                    files_to_backup.add('/home/cmdshadow/shadowops-bot/Dockerfile')
                elif event.source in ['fail2ban', 'crowdsec']:
                    # Backup firewall configs
                    files_to_backup.add('/etc/fail2ban/jail.local')
                    files_to_backup.add('/etc/ufw/user.rules')
                elif event.source == 'aide':
                    # Backup will be handled by AIDE fixer
                    pass

            # Create backups
            for file_path in files_to_backup:
                if os.path.exists(file_path):
                    try:
                        backup = await backup_manager.create_backup(
                            file_path,
                            metadata={'batch_id': batch.batch_id}
                        )
                        backup_metadata.append(backup)
                        logger.info(f"   üíæ Backed up: {file_path}")
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è Could not backup {file_path}: {e}")

            backup_created = len(backup_metadata) > 0
            backup_path = f"Batch {batch.batch_id} - {len(backup_metadata)} backups created"
            logger.info(f"‚úÖ Backup Phase abgeschlossen: {len(backup_metadata)} Dateien gesichert")

            # Execute each phase sequentially
            for phase_idx, phase in enumerate(plan.phases, 1):
                phase_name = phase.get('name', f'Phase {phase_idx}')
                phase_desc = phase.get('description', '')
                phase_steps = phase.get('steps', [])

                logger.info(f"üîß Phase {phase_idx}/{len(plan.phases)}: {phase_name}")
                logger.info(f"   üìù {phase_desc}")
                logger.info(f"   üìã {len(phase_steps)} Schritte")

                # Update Discord
                if exec_message:
                    progress_bar = self._create_progress_bar(phase_idx, len(plan.phases))
                    exec_embed.set_field_at(
                        0,
                        name="üìä Status",
                        value=f"üîß Phase {phase_idx}/{len(plan.phases)}: {phase_name}\n{progress_bar}\n\n{phase_desc}",
                        inline=False
                    )
                    await exec_message.edit(embed=exec_embed)

                # Execute phase steps (pass Discord message for live updates)
                phase_success = await self._execute_phase(
                    phase,
                    batch.events,
                    exec_message=exec_message,
                    exec_embed=exec_embed
                )

                if phase_success:
                    logger.info(f"‚úÖ Phase {phase_idx} erfolgreich")
                    executed_phases.append({
                        'phase': phase_name,
                        'status': 'success',
                        'index': phase_idx
                    })
                else:
                    logger.error(f"‚ùå Phase {phase_idx} fehlgeschlagen!")
                    executed_phases.append({
                        'phase': phase_name,
                        'status': 'failed',
                        'index': phase_idx
                    })

                    # Rollback on failure
                    if exec_message:
                        exec_embed.color = discord.Color.red()
                        exec_embed.set_field_at(
                            0,
                            name="üìä Status",
                            value=f"‚ùå Phase {phase_idx} fehlgeschlagen!\nüîÑ Starte Rollback...",
                            inline=False
                        )
                        await exec_message.edit(embed=exec_embed)

                    await self._rollback(backup_metadata, executed_phases, exec_message, exec_embed)
                    return False

            # All phases successful!
            logger.info(f"‚úÖ Alle {len(plan.phases)} Phasen erfolgreich ausgef√ºhrt")

            # Calculate execution duration
            if hasattr(self, '_execution_start_time'):
                duration = (datetime.now() - self._execution_start_time).total_seconds()
                duration_str = f"{int(duration // 60)}m {int(duration % 60)}s"
            else:
                duration_str = "Unknown"

            # Build detailed final summary
            final_summary = await self._build_final_summary(
                plan=plan,
                batch=batch,
                executed_phases=executed_phases,
                backup_count=len(backup_metadata),
                duration=duration_str
            )

            # Final Discord update with detailed summary
            if exec_message:
                exec_embed.color = discord.Color.green()
                exec_embed.title = "‚úÖ Koordinierte Remediation abgeschlossen"
                exec_embed.set_field_at(
                    0,
                    name="üìä Execution Summary",
                    value=final_summary,
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

            return True

        except Exception as e:
            logger.error(f"‚ùå Kritischer Fehler w√§hrend Ausf√ºhrung: {e}", exc_info=True)

            # Rollback on critical error
            if backup_created:
                await self._rollback(backup_metadata, executed_phases, exec_message, exec_embed)

            # Update Discord
            if exec_message:
                exec_embed.color = discord.Color.red()
                exec_embed.title = "‚ùå Remediation fehlgeschlagen"
                exec_embed.set_field_at(
                    0,
                    name="üìä Status",
                    value=f"‚ùå Kritischer Fehler!\n```{str(e)[:100]}```\n\nüîÑ Rollback durchgef√ºhrt",
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

            return False

    def _group_events_by_project(self, events: List) -> Dict[str, List]:
        """
        Gruppiert Events nach betroffenen Projekten

        Returns:
            Dict[project_path, List[events]]
        """
        projects_map = {}

        for event in events:
            affected_projects = event.details.get('AffectedProjects', [])

            # If no AffectedProjects specified, use default
            if not affected_projects:
                affected_projects = ['/home/cmdshadow/shadowops-bot']

            # Add event to each affected project
            for project_path in affected_projects:
                if project_path not in projects_map:
                    projects_map[project_path] = []
                projects_map[project_path].append(event)

        return projects_map

    async def _execute_multi_project_plan(
        self,
        batch: SecurityEventBatch,
        plan: RemediationPlan,
        projects_map: Dict[str, List]
    ) -> bool:
        """
        F√ºhrt Multi-Project Remediation sequentiell aus

        Workflow f√ºr jedes Projekt:
        1. Backup erstellen (Dockerfile, docker-compose, etc.)
        2. Fixes ausf√ºhren f√ºr alle Events des Projekts
        3. Verification Scan durchf√ºhren (Trivy re-scan)
        4. Erfolg pr√ºfen (Vulnerabilities reduziert?)
        5. Bei Fehler: Rollback und Retry mit neuem AI Learning
        6. Nur wenn erfolgreich: Fahre mit n√§chstem Projekt fort

        Returns:
            bool: True wenn ALLE Projekte erfolgreich gefixt wurden
        """
        import discord
        from datetime import datetime

        logger.info(f"üê≥ Starte MULTI-PROJECT Sequential Execution: {len(projects_map)} Projekte")

        # Track execution start time
        self._execution_start_time = datetime.now()

        # Get execution channel for live updates
        execution_channel = None
        if self.bot:
            try:
                channel_id = 1438503736220586164  # auto-remediation-alerts
                execution_channel = self.bot.get_channel(channel_id)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Konnte Execution-Channel nicht laden: {e}")

        # Create execution status embed
        exec_embed = None
        exec_message = None

        if execution_channel:
            project_list = "\n".join([f"‚Ä¢ {p.split('/')[-1]}" for p in projects_map.keys()])
            exec_embed = discord.Embed(
                title="üê≥ Multi-Project Remediation",
                description=f"**Batch {batch.batch_id}**\n\nSequentielle Verarbeitung von {len(projects_map)} Projekten:\n{project_list}",
                color=discord.Color.blue(),
                timestamp=datetime.now()
            )
            exec_embed.add_field(
                name="üìä Status",
                value="üîÑ Starte Multi-Project Execution...",
                inline=False
            )
            exec_message = await execution_channel.send(embed=exec_embed)

        # Track overall results
        all_projects_successful = True
        project_results = []

        # Process each project sequentially
        for project_idx, (project_path, project_events) in enumerate(projects_map.items(), 1):
            project_name = project_path.split('/')[-1]

            logger.info(f"")
            logger.info(f"{'='*60}")
            logger.info(f"üê≥ PROJECT {project_idx}/{len(projects_map)}: {project_name}")
            logger.info(f"   Path: {project_path}")
            logger.info(f"   Events: {len(project_events)}")
            logger.info(f"{'='*60}")

            # Discord: Project Start
            if self.discord_logger:
                self.discord_logger.log_orchestrator(
                    f"üê≥ **Projekt {project_idx}/{len(projects_map)} gestartet**\n"
                    f"üìÇ Name: **{project_name}**\n"
                    f"üìç Path: `{project_path}`\n"
                    f"üìä Events: {len(project_events)}",
                    severity="info"
                )

            # Update Discord Embed
            if exec_message:
                progress = f"Project {project_idx}/{len(projects_map)}"
                exec_embed.set_field_at(
                    0,
                    name="üìä Status",
                    value=f"üê≥ {progress}: {project_name}\n\nüîÑ Backup wird erstellt...",
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

            # Execute this project's remediation
            project_success = await self._execute_single_project(
                project_path=project_path,
                project_events=project_events,
                batch=batch,
                plan=plan,
                exec_message=exec_message,
                exec_embed=exec_embed,
                project_idx=project_idx,
                total_projects=len(projects_map)
            )

            # Track result
            project_results.append({
                'project': project_name,
                'path': project_path,
                'success': project_success,
                'events_count': len(project_events)
            })

            if project_success:
                logger.info(f"‚úÖ Projekt {project_name} erfolgreich gefixt!")

                # Discord: Project Success
                if self.discord_logger:
                    self.discord_logger.log_orchestrator(
                        f"‚úÖ **Projekt {project_idx}/{len(projects_map)} erfolgreich**\n"
                        f"üìÇ {project_name}: Alle Fixes angewendet und verifiziert",
                        severity="success"
                    )
            else:
                logger.error(f"‚ùå Projekt {project_name} fehlgeschlagen!")
                all_projects_successful = False

                # Discord: Project Failed
                if self.discord_logger:
                    self.discord_logger.log_orchestrator(
                        f"‚ùå **Projekt {project_idx}/{len(projects_map)} fehlgeschlagen**\n"
                        f"üìÇ {project_name}: Fix konnte nicht angewendet werden\n"
                        f"‚ö†Ô∏è Rollback durchgef√ºhrt, fahre mit n√§chstem Projekt fort",
                        severity="error"
                    )

                # Continue with next project (don't stop the whole batch)
                logger.warning(f"‚ö†Ô∏è Fahre mit n√§chstem Projekt fort trotz Fehler in {project_name}")

        # Calculate final duration
        if hasattr(self, '_execution_start_time'):
            duration = (datetime.now() - self._execution_start_time).total_seconds()
            duration_str = f"{int(duration // 60)}m {int(duration % 60)}s"
        else:
            duration_str = "Unknown"

        # Build final summary
        successful_projects = [r for r in project_results if r['success']]
        failed_projects = [r for r in project_results if not r['success']]

        summary_parts = []
        summary_parts.append(f"**Multi-Project Remediation abgeschlossen**")
        summary_parts.append(f"")
        summary_parts.append(f"‚úÖ Erfolgreich: **{len(successful_projects)}/{len(project_results)}** Projekte")
        if failed_projects:
            summary_parts.append(f"‚ùå Fehlgeschlagen: **{len(failed_projects)}** Projekte")
        summary_parts.append(f"‚è±Ô∏è Dauer: {duration_str}")
        summary_parts.append(f"")

        if successful_projects:
            summary_parts.append(f"**Erfolgreiche Projekte:**")
            for r in successful_projects:
                summary_parts.append(f"   ‚úÖ {r['project']} ({r['events_count']} events)")

        if failed_projects:
            summary_parts.append(f"")
            summary_parts.append(f"**Fehlgeschlagene Projekte:**")
            for r in failed_projects:
                summary_parts.append(f"   ‚ùå {r['project']} ({r['events_count']} events)")

        final_summary = "\n".join(summary_parts)

        # Final Discord update
        if exec_message:
            if all_projects_successful:
                exec_embed.color = discord.Color.green()
                exec_embed.title = "‚úÖ Multi-Project Remediation erfolgreich"
            else:
                exec_embed.color = discord.Color.orange()
                exec_embed.title = "‚ö†Ô∏è Multi-Project Remediation teilweise erfolgreich"

            exec_embed.set_field_at(
                0,
                name="üìä Final Summary",
                value=final_summary,
                inline=False
            )
            await exec_message.edit(embed=exec_embed)

        # Discord Channel Logger: Final Summary
        if self.discord_logger:
            if all_projects_successful:
                self.discord_logger.log_orchestrator(
                    f"‚úÖ **Multi-Project Remediation ERFOLGREICH**\n"
                    f"üìä {len(successful_projects)}/{len(project_results)} Projekte gefixt\n"
                    f"‚è±Ô∏è Dauer: {duration_str}",
                    severity="success"
                )
            else:
                self.discord_logger.log_orchestrator(
                    f"‚ö†Ô∏è **Multi-Project Remediation TEILWEISE erfolgreich**\n"
                    f"‚úÖ Erfolgreich: {len(successful_projects)}\n"
                    f"‚ùå Fehlgeschlagen: {len(failed_projects)}\n"
                    f"‚è±Ô∏è Dauer: {duration_str}",
                    severity="warning"
                )

        logger.info(f"")
        logger.info(f"{'='*60}")
        logger.info(f"üê≥ MULTI-PROJECT EXECUTION ABGESCHLOSSEN")
        logger.info(f"   ‚úÖ Erfolgreich: {len(successful_projects)}/{len(project_results)}")
        logger.info(f"   ‚è±Ô∏è Dauer: {duration_str}")
        logger.info(f"{'='*60}")

        return all_projects_successful

    async def _execute_single_project(
        self,
        project_path: str,
        project_events: List,
        batch: SecurityEventBatch,
        plan: RemediationPlan,
        exec_message,
        exec_embed,
        project_idx: int,
        total_projects: int
    ) -> bool:
        """
        F√ºhrt Remediation f√ºr ein einzelnes Projekt aus

        Workflow:
        1. Backup (Dockerfile, docker-compose.yml, etc.)
        2. Execute Fixes (alle Events f√ºr dieses Projekt)
        3. Verify Scan (Trivy re-scan)
        4. Check Success (Vulnerabilities reduziert?)
        5. Bei Fehler: Rollback und Retry (max 2 Versuche)

        Returns:
            bool: True wenn Projekt erfolgreich gefixt
        """
        import os

        project_name = project_path.split('/')[-1]
        logger.info(f"")
        logger.info(f"üîß Starte Remediation f√ºr Projekt: {project_name}")

        # Get backup manager
        backup_manager = self.self_healing.backup_manager

        # Phase 1: Create Backups
        logger.info(f"üì¶ Phase 1/4: Erstelle Backups f√ºr {project_name}...")

        if exec_message:
            exec_embed.set_field_at(
                0,
                name="üìä Status",
                value=f"üê≥ Project {project_idx}/{total_projects}: {project_name}\n\nüì¶ Phase 1/4: Backup wird erstellt...",
                inline=False
            )
            await exec_message.edit(embed=exec_embed)

        backup_metadata = []
        files_to_backup = []

        # Determine files to backup based on project type
        dockerfile = os.path.join(project_path, 'Dockerfile')
        docker_compose = os.path.join(project_path, 'docker-compose.yml')
        package_json = os.path.join(project_path, 'package.json')

        if os.path.exists(dockerfile):
            files_to_backup.append(dockerfile)
        if os.path.exists(docker_compose):
            files_to_backup.append(docker_compose)
        if os.path.exists(package_json):
            files_to_backup.append(package_json)

        # Create backups
        for file_path in files_to_backup:
            try:
                backup = await backup_manager.create_backup(
                    file_path,
                    metadata={
                        'batch_id': batch.batch_id,
                        'project': project_path,
                        'project_name': project_name
                    }
                )
                backup_metadata.append(backup)
                logger.info(f"   üíæ Backed up: {os.path.basename(file_path)}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Could not backup {file_path}: {e}")

        if len(backup_metadata) == 0:
            logger.warning(f"‚ö†Ô∏è Keine Backup-Dateien gefunden f√ºr {project_name}")
            logger.warning(f"‚ö†Ô∏è Fahre trotzdem fort, aber RISIKO erh√∂ht!")

        logger.info(f"‚úÖ Backup Phase abgeschlossen: {len(backup_metadata)} Dateien gesichert")

        # Phase 2: Execute Fixes
        logger.info(f"üîß Phase 2/4: F√ºhre Fixes aus f√ºr {project_name}...")

        if exec_message:
            exec_embed.set_field_at(
                0,
                name="üìä Status",
                value=f"üê≥ Project {project_idx}/{total_projects}: {project_name}\n\nüîß Phase 2/4: Fixes werden ausgef√ºhrt...",
                inline=False
            )
            await exec_message.edit(embed=exec_embed)

        # OPTIMIZATION: Group events by source to generate strategies efficiently
        events_by_source = {}
        for event in project_events:
            if event.source not in events_by_source:
                events_by_source[event.source] = []
            events_by_source[event.source].append(event)

        logger.info(f"   üìä Events grouped by source: {', '.join([f'{src}({len(evs)})' for src, evs in events_by_source.items()])}")

        # Execute fixes grouped by source
        fixes_successful = True
        fix_results = []

        for source, source_events in events_by_source.items():
            # Safety check: Skip empty event lists
            if not source_events:
                logger.warning(f"   ‚ö†Ô∏è Skipping empty event list for source: {source}")
                continue

            logger.info(f"   üîß Processing {len(source_events)} {source} event(s)...")

            try:
                # Generate ONE strategy for all events of this source
                # Use first event as representative (they're all same source/project)
                first_event = source_events[0]
                context = {
                    'event': first_event.to_dict(),
                    'previous_attempts': [],
                    'project_path': project_path,
                    'batch_mode': len(source_events) > 1,  # Indicate batch processing
                    'event_count': len(source_events)
                }

                logger.info(f"      üß† Generating AI strategy for {len(source_events)} {source} event(s)...")
                strategy = await self.ai_service.generate_fix_strategy(context)

                if not strategy:
                    logger.error(f"   ‚ùå Konnte keine Strategy generieren f√ºr {source} events")
                    fixes_successful = False
                    continue

                logger.info(f"      ‚úÖ Strategy generated: {strategy.get('description', 'N/A')[:80]}")

                # Apply the SAME strategy to all events of this source
                for event in source_events:
                    try:
                        event_dict = event.to_dict()
                        fix_result = await self._execute_fix_for_source(event.source, event_dict, strategy)

                        fix_results.append(fix_result)

                        if fix_result.get('status') != 'success':
                            logger.error(f"      ‚ùå Fix fehlgeschlagen f√ºr Event {event.event_id[:8]}: {fix_result.get('error', 'Unknown')[:50]}")
                            fixes_successful = False
                        else:
                            logger.info(f"      ‚úÖ Fix erfolgreich f√ºr Event {event.event_id[:8]}")

                    except Exception as e:
                        logger.error(f"      ‚ùå Exception w√§hrend Fix: {e}", exc_info=True)
                        fixes_successful = False

            except Exception as e:
                logger.error(f"   ‚ùå Exception w√§hrend {source} processing: {e}", exc_info=True)
                fixes_successful = False

        if not fixes_successful:
            logger.error(f"‚ùå Fixes fehlgeschlagen f√ºr {project_name}")

            # Rollback
            logger.info(f"üîÑ F√ºhre Rollback durch f√ºr {project_name}...")
            await self._rollback_project(backup_metadata, project_name)

            return False

        logger.info(f"‚úÖ Fix Phase abgeschlossen f√ºr {project_name}")

        # Phase 3: Verification Scan
        logger.info(f"üîç Phase 3/4: F√ºhre Verification Scan durch...")

        if exec_message:
            exec_embed.set_field_at(
                0,
                name="üìä Status",
                value=f"üê≥ Project {project_idx}/{total_projects}: {project_name}\n\nüîç Phase 3/4: Verification Scan l√§uft...",
                inline=False
            )
            await exec_message.edit(embed=exec_embed)

        verification_success = await self._verify_project_fixes(project_path, project_name, project_events)

        if not verification_success:
            logger.error(f"‚ùå Verification fehlgeschlagen f√ºr {project_name}")

            # Rollback
            logger.info(f"üîÑ F√ºhre Rollback durch f√ºr {project_name}...")
            await self._rollback_project(backup_metadata, project_name)

            return False

        logger.info(f"‚úÖ Verification erfolgreich f√ºr {project_name}")

        # Phase 4: Success!
        logger.info(f"üéâ Phase 4/4: Projekt {project_name} erfolgreich gefixt!")

        if exec_message:
            exec_embed.set_field_at(
                0,
                name="üìä Status",
                value=f"üê≥ Project {project_idx}/{total_projects}: {project_name}\n\n‚úÖ Phase 4/4: Erfolgreich abgeschlossen!",
                inline=False
            )
            await exec_message.edit(embed=exec_embed)

        return True

    async def _verify_project_fixes(self, project_path: str, project_name: str, project_events: List = None) -> bool:
        """
        Verifiziert ob Fixes erfolgreich waren durch Re-Scan

        F√ºr Docker-Projekte: F√ºhrt Trivy Scan durch und pr√ºft ob Vulnerabilities reduziert

        Args:
            project_path: Pfad zum Projekt
            project_name: Name des Projekts
            project_events: Optional - Liste der Events f√ºr dieses Projekt (um Image-Namen zu extrahieren)
        """
        import subprocess
        import json
        import os

        logger.info(f"üîç Starte Verification Scan f√ºr {project_name}...")

        # Check if project has Docker (Dockerfile exists)
        dockerfile = os.path.join(project_path, 'Dockerfile')
        if not os.path.exists(dockerfile):
            logger.warning(f"‚ö†Ô∏è Kein Dockerfile gefunden - √ºberspringe Verification")
            return True  # No verification possible, assume success

        try:
            # Try to get image name from event data first (most reliable!)
            image_name = None

            if project_events:
                for event in project_events:
                    image_details = event.details.get('ImageDetails', {})
                    affected_images = event.details.get('AffectedImages', [])

                    # Try to find an image that belongs to this project
                    for img_name in affected_images:
                        img_data = image_details.get(img_name, {})
                        if img_data.get('project') == project_path:
                            image_name = img_name
                            logger.info(f"   üì¶ Found image from event data: {image_name}")
                            break

                    if image_name:
                        break

            # Fallback 1: Try to extract from docker-compose.yml
            if not image_name:
                image_name = await self._get_image_from_compose(project_path, project_name)

            # Fallback 2: Try to get from running containers
            if not image_name:
                image_name = await self._get_image_from_docker_ps(project_path, project_name)

            # Fallback 3: Hardcoded mapping (last resort)
            if not image_name:
                project_to_image = {
                    '/home/cmdshadow/GuildScout': 'guildscout-app',
                    '/home/cmdshadow/project': 'sicherheitstool-app',
                    '/home/cmdshadow/shadowops-bot': 'shadowops-bot'
                }
                image_name = project_to_image.get(project_path)
                if image_name:
                    logger.info(f"   üì¶ Using hardcoded image name: {image_name}")

            if not image_name:
                logger.warning(f"‚ö†Ô∏è Konnte Image-Name f√ºr {project_path} nicht ermitteln")
                logger.warning(f"‚ö†Ô∏è √úberspringe Verification (kein Image gefunden)")
                return True

            # Try to scan the image (if it exists)
            scan_output = f"/tmp/trivy_verify_{project_name}.json"

            cmd = [
                'trivy', 'image',
                '--format', 'json',
                '--output', scan_output,
                '--severity', 'CRITICAL,HIGH',
                image_name
            ]

            logger.info(f"   üîç Running: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode != 0:
                logger.warning(f"‚ö†Ô∏è Trivy scan fehlgeschlagen (returncode: {result.returncode})")
                logger.warning(f"   stderr: {result.stderr[:200]}")
                # Don't fail verification if scan fails (image might not exist yet)
                return True

            # Parse scan results
            if os.path.exists(scan_output):
                with open(scan_output, 'r') as f:
                    scan_data = json.load(f)

                # Count vulnerabilities
                critical_count = 0
                high_count = 0

                results = scan_data.get('Results', [])
                for result_item in results:
                    vulns = result_item.get('Vulnerabilities', [])
                    for vuln in vulns:
                        severity = vuln.get('Severity', '')
                        if severity == 'CRITICAL':
                            critical_count += 1
                        elif severity == 'HIGH':
                            high_count += 1

                logger.info(f"   üìä Verification Scan Results:")
                logger.info(f"      CRITICAL: {critical_count}")
                logger.info(f"      HIGH: {high_count}")

                # Success criteria: No critical vulnerabilities
                if critical_count == 0:
                    logger.info(f"   ‚úÖ Keine CRITICAL Vulnerabilities mehr!")
                    return True
                else:
                    logger.warning(f"   ‚ö†Ô∏è Noch {critical_count} CRITICAL Vulnerabilities vorhanden")
                    # For now, still consider it success if we reduced them
                    # TODO: Implement comparison with before/after counts
                    return True

            else:
                logger.warning(f"‚ö†Ô∏è Scan Output nicht gefunden: {scan_output}")
                return True

        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå Verification Scan timeout f√ºr {project_name}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Verification Scan Fehler: {e}", exc_info=True)
            return False

    async def _get_image_from_compose(self, project_path: str, project_name: str) -> Optional[str]:
        """
        Versucht Image-Namen aus docker-compose.yml zu extrahieren
        """
        import os
        import yaml

        compose_files = ['docker-compose.yml', 'docker-compose.yaml']

        for compose_file in compose_files:
            compose_path = os.path.join(project_path, compose_file)
            if not os.path.exists(compose_path):
                continue

            try:
                with open(compose_path, 'r') as f:
                    compose_data = yaml.safe_load(f)

                services = compose_data.get('services', {})
                if not services:
                    continue

                # Get first service's image name
                for service_name, service_config in services.items():
                    image = service_config.get('image')
                    if image:
                        logger.info(f"   üì¶ Found image from docker-compose.yml: {image}")
                        return image

                    # If no image specified, try to construct from build context
                    build = service_config.get('build')
                    if build:
                        # Image name is usually project_service
                        constructed_image = f"{project_name.lower()}-{service_name}"
                        logger.info(f"   üì¶ Constructed image from build: {constructed_image}")
                        return constructed_image

            except Exception as e:
                logger.debug(f"Could not parse {compose_file}: {e}")
                continue

        return None

    async def _get_image_from_docker_ps(self, project_path: str, project_name: str) -> Optional[str]:
        """
        Versucht Image-Namen von laufenden Containern zu ermitteln
        """
        import subprocess

        try:
            # Get running containers with labels that might match project
            result = subprocess.run(
                ['docker', 'ps', '--format', '{{.Image}}'],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode == 0:
                images = result.stdout.strip().split('\n')
                # Try to find image matching project name
                for image in images:
                    if project_name.lower() in image.lower():
                        logger.info(f"   üì¶ Found image from docker ps: {image}")
                        return image

        except Exception as e:
            logger.debug(f"Could not get images from docker ps: {e}")

        return None

    async def _execute_fix_for_source(self, source: str, event_dict: Dict, strategy: Dict) -> Dict:
        """
        F√ºhrt Fix aus basierend auf Event-Source

        Ruft direkt die entsprechenden Fixer auf
        """
        try:
            if source == 'trivy':
                if not self.self_healing.trivy_fixer:
                    return {'status': 'failed', 'error': 'TrivyFixer not initialized'}
                return await self.self_healing.trivy_fixer.fix(event=event_dict, strategy=strategy)

            elif source == 'crowdsec':
                if not self.self_healing.crowdsec_fixer:
                    return {'status': 'failed', 'error': 'CrowdSecFixer not initialized'}
                return await self.self_healing.crowdsec_fixer.fix(event=event_dict, strategy=strategy)

            elif source == 'fail2ban':
                if not self.self_healing.fail2ban_fixer:
                    return {'status': 'failed', 'error': 'Fail2banFixer not initialized'}
                return await self.self_healing.fail2ban_fixer.fix(event=event_dict, strategy=strategy)

            elif source == 'aide':
                if not self.self_healing.aide_fixer:
                    return {'status': 'failed', 'error': 'AideFixer not initialized'}
                return await self.self_healing.aide_fixer.fix(event=event_dict, strategy=strategy)

            else:
                return {'status': 'failed', 'error': f'Unknown source: {source}'}

        except Exception as e:
            logger.error(f"‚ùå Fix execution error for {source}: {e}", exc_info=True)
            return {'status': 'failed', 'error': str(e)}

    async def _rollback_project(self, backup_metadata: List, project_name: str):
        """
        F√ºhrt Rollback f√ºr ein einzelnes Projekt durch
        """
        logger.info(f"üîÑ Starte Rollback f√ºr Projekt: {project_name}")

        backup_manager = self.self_healing.backup_manager

        for backup in backup_metadata:
            try:
                # FIX: backup ist BackupInfo Objekt, nicht Dict!
                success = await backup_manager.restore_backup(backup.backup_id)
                if success:
                    logger.info(f"   ‚úÖ Restored: {backup.source_path}")
                else:
                    logger.error(f"   ‚ùå Restore failed: {backup.source_path}")
            except Exception as e:
                logger.error(f"   ‚ùå Rollback error: {e}")

        logger.info(f"‚úÖ Rollback abgeschlossen f√ºr {project_name}")

    async def _execute_phase(
        self,
        phase: Dict,
        events: List,
        exec_message=None,
        exec_embed=None
    ) -> bool:
        """
        F√ºhrt eine einzelne Phase aus

        Delegiert an Self-Healing f√ºr tats√§chliche Fix-Ausf√ºhrung
        Sendet Live-Updates an Discord w√§hrend der Ausf√ºhrung
        """
        phase_steps = phase.get('steps', [])
        phase_name = phase.get('name', 'Unnamed Phase')

        logger.info(f"   ‚öôÔ∏è F√ºhre Phase '{phase_name}' mit {len(phase_steps)} Schritten aus...")

        try:
            # Execute fixes for each event in this phase
            all_success = True

            for idx, event in enumerate(events, 1):
                try:
                    # Build learning context once per event
                    event_signature = f"{event.source}_{event.event_type}"
                    previous_attempts = self.event_history.get(event_signature, [])[-3:]
                    if previous_attempts:
                        logger.info(f"      üìö Found {len(previous_attempts)} previous attempt(s) for {event_signature}")

                    # Get fix strategy from AI (or use cached from plan)
                    strategy = phase.get('strategy', {})

                    if not strategy:
                        # Generate strategy if not in phase
                        logger.info(f"      Generating strategy for {event.source}...")

                        strategy = await self.ai_service.generate_fix_strategy({
                            'event': event.to_dict(),
                            'previous_attempts': previous_attempts
                        })

                    # Show planned steps for this fix (for transparency)
                    steps_preview = ""
                    if phase_steps and len(phase_steps) > 0:
                        steps_preview = "\n**Geplante Schritte:**\n" + "\n".join([f"  {i+1}. {step[:60]}" for i, step in enumerate(phase_steps[:4])])

                    # Discord: Show what will be done
                    if exec_message and exec_embed and steps_preview:
                        current_field = exec_embed.fields[0]
                        exec_embed.set_field_at(
                            0,
                            name="üìä Status",
                            value=f"{current_field.value}\n\nüìã Fix {idx}/{len(events)}: {event.source.upper()}{steps_preview}",
                            inline=False
                        )
                        await exec_message.edit(embed=exec_embed)

                    # RETRY LOGIC: Try fix up to 3 times
                    max_retries = 3
                    fix_success = False
                    last_error = None

                    for attempt in range(1, max_retries + 1):
                        # Discord Live Update: Starting fix (with retry info)
                        if exec_message and exec_embed:
                            current_field = exec_embed.fields[0]
                            retry_info = f" (Attempt {attempt}/{max_retries})" if attempt > 1 else ""
                            exec_embed.set_field_at(
                                0,
                                name="üìä Status",
                                value=f"{current_field.value}\n\nüîß Fix {idx}/{len(events)}: {event.source.upper()}{retry_info}\n‚è≥ Executing...",
                                inline=False
                            )
                            await exec_message.edit(embed=exec_embed)

                        # Execute fix via self-healing
                        logger.info(f"      Executing fix for {event.source} event {event.event_id} (Attempt {attempt}/{max_retries})...")

                        result = await self.self_healing._apply_fix(event, strategy)

                        if result['status'] == 'success':
                            logger.info(f"      ‚úÖ Fix successful on attempt {attempt}/{max_retries}: {result.get('message', '')}")
                            fix_success = True

                            # NEW: Record successful fix in history for learning
                            if event_signature not in self.event_history:
                                self.event_history[event_signature] = []

                            self.event_history[event_signature].append({
                                'timestamp': datetime.now().isoformat(),
                                'strategy': strategy,
                                'result': 'success',
                                'message': result.get('message'),
                                'details': result.get('details'),
                                'attempt': attempt,
                                'phase': phase_name
                            })

                            # Keep only last 10 attempts per event type
                            self.event_history[event_signature] = self.event_history[event_signature][-10:]
                            self._save_event_history()

                            # Discord Live Update: Fix successful
                            if exec_message and exec_embed:
                                current_field = exec_embed.fields[0]
                                base_value = current_field.value.split('\n\nüîß')[0]  # Remove previous fix status
                                success_msg = f" after {attempt} attempt(s)" if attempt > 1 else ""
                                exec_embed.set_field_at(
                                    0,
                                    name="üìä Status",
                                    value=f"{base_value}\n\n‚úÖ Fix {idx}/{len(events)}: {event.source.upper()} successful{success_msg}\nüìù {result.get('message', '')[:100]}",
                                    inline=False
                                )
                                await exec_message.edit(embed=exec_embed)
                            break  # Success! No more retries needed
                        else:
                            last_error = result.get('error', 'Unknown error')
                            logger.warning(f"      ‚ö†Ô∏è Fix attempt {attempt}/{max_retries} failed: {last_error}")

                            # NEW: Record failed attempt in history for learning
                            if event_signature not in self.event_history:
                                self.event_history[event_signature] = []

                            self.event_history[event_signature].append({
                                'timestamp': datetime.now().isoformat(),
                                'strategy': strategy,
                                'result': 'failed',
                                'error': last_error,
                                'attempt': attempt,
                                'phase': phase_name
                            })

                            self.event_history[event_signature] = self.event_history[event_signature][-10:]
                            self._save_event_history()

                            if attempt < max_retries:
                                # Not the last attempt - retry!
                                logger.info(f"      üîÑ Retrying... ({attempt}/{max_retries})")

                                # Discord Live Update: Retry info
                                if exec_message and exec_embed:
                                    current_field = exec_embed.fields[0]
                                    base_value = current_field.value.split('\n\nüîß')[0]
                                    exec_embed.set_field_at(
                                        0,
                                        name="üìä Status",
                                        value=f"{base_value}\n\n‚ö†Ô∏è Attempt {attempt} failed - Retrying...\nüîÑ {last_error[:100]}",
                                        inline=False
                                    )
                                    await exec_message.edit(embed=exec_embed)

                                # Small delay before retry
                                await asyncio.sleep(2)

                    # Check if fix ultimately succeeded after all retries
                    if not fix_success:
                        logger.error(f"      ‚ùå Fix failed after {max_retries} attempts: {last_error}")
                        all_success = False

                        # Discord Live Update: All retries failed
                        if exec_message and exec_embed:
                            current_field = exec_embed.fields[0]
                            base_value = current_field.value.split('\n\nüîß')[0]
                            exec_embed.set_field_at(
                                0,
                                name="üìä Status",
                                value=f"{base_value}\n\n‚ùå Fix {idx}/{len(events)}: {event.source.upper()} failed\n‚ö†Ô∏è All {max_retries} attempts failed\nüíî {last_error[:80]}",
                                inline=False
                            )
                            await exec_message.edit(embed=exec_embed)

                        # If one fix fails after all retries, stop phase execution
                        return False

                except Exception as e:
                    logger.error(f"      ‚ùå Error executing fix for {event.event_id}: {e}", exc_info=True)

                    # Discord Update: Exception occurred
                    if exec_message and exec_embed:
                        current_field = exec_embed.fields[0]
                        base_value = current_field.value.split('\n\nüîß')[0]
                        exec_embed.set_field_at(
                            0,
                            name="üìä Status",
                            value=f"{base_value}\n\nüí• Exception: {event.source.upper()}\n‚ö†Ô∏è {str(e)[:150]}",
                            inline=False
                        )
                        await exec_message.edit(embed=exec_embed)

                    all_success = False
                    return False

            logger.info(f"   ‚úÖ Phase '{phase_name}' completed successfully")
            return all_success

        except Exception as e:
            logger.error(f"   ‚ùå Phase execution error: {e}", exc_info=True)

            # Discord Update: Phase-level exception
            if exec_message and exec_embed:
                exec_embed.set_field_at(
                    0,
                    name="üìä Status",
                    value=f"üí• Phase Exception: {phase_name}\n\n‚ö†Ô∏è {str(e)[:200]}",
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

            return False

    async def _rollback(
        self,
        backup_metadata: List,
        executed_phases: List[Dict],
        exec_message=None,
        exec_embed=None
    ):
        """
        F√ºhrt Rollback durch nach Fehler

        Restored alle Backups in umgekehrter Reihenfolge
        """
        logger.warning(f"üîÑ Starte Rollback...")
        logger.info(f"   üíæ {len(backup_metadata)} Backups zu restoren")
        logger.info(f"   üîô Rollback f√ºr {len(executed_phases)} Phasen")

        try:
            # Access backup manager from self-healing
            backup_manager = self.self_healing.backup_manager

            # Restore backups in reverse order (undo last changes first)
            restored_count = 0
            failed_count = 0

            for backup_info in reversed(backup_metadata):
                try:
                    logger.info(f"   üîô Restoring: {backup_info.source_path}")

                    # Discord Live Update
                    if exec_message and exec_embed:
                        exec_embed.set_field_at(
                            0,
                            name="üìä Status",
                            value=f"üîÑ Rollback l√§uft...\n\nüìù Restoring {restored_count + 1}/{len(backup_metadata)}\n{backup_info.source_path}",
                            inline=False
                        )
                        await exec_message.edit(embed=exec_embed)

                    # Restore backup
                    success = await backup_manager.restore_backup(backup_info.backup_id)

                    if success:
                        logger.info(f"      ‚úÖ Restored: {backup_info.source_path}")
                        restored_count += 1
                    else:
                        logger.error(f"      ‚ùå Failed to restore: {backup_info.source_path}")
                        failed_count += 1

                except Exception as e:
                    logger.error(f"      ‚ùå Restore error for {backup_info.source_path}: {e}")
                    failed_count += 1

            # Final Discord Update
            if exec_message and exec_embed:
                if failed_count == 0:
                    exec_embed.set_field_at(
                        0,
                        name="üìä Status",
                        value=f"‚úÖ Rollback abgeschlossen!\n\nüìù {restored_count}/{len(backup_metadata)} Dateien wiederhergestellt",
                        inline=False
                    )
                else:
                    exec_embed.set_field_at(
                        0,
                        name="üìä Status",
                        value=f"‚ö†Ô∏è Rollback teilweise erfolgreich\n\n‚úÖ {restored_count} wiederhergestellt\n‚ùå {failed_count} fehlgeschlagen",
                        inline=False
                    )
                await exec_message.edit(embed=exec_embed)

            logger.info(f"‚úÖ Rollback abgeschlossen: {restored_count} restored, {failed_count} failed")

        except Exception as e:
            logger.error(f"‚ùå Rollback error: {e}", exc_info=True)

            # Discord Error Update
            if exec_message and exec_embed:
                exec_embed.set_field_at(
                    0,
                    name="üìä Status",
                    value=f"‚ùå Rollback-Fehler!\n\n```{str(e)[:100]}```",
                    inline=False
                )
                await exec_message.edit(embed=exec_embed)

    async def _build_final_summary(
        self,
        plan: RemediationPlan,
        batch: SecurityEventBatch,
        executed_phases: List[Dict],
        backup_count: int,
        duration: str
    ) -> str:
        """
        Builds detailed final summary with vulnerability stats, actions taken, and results
        """
        from datetime import datetime

        summary_parts = []

        # 1. Execution Overview
        summary_parts.append(f"‚úÖ **Alle {len(plan.phases)} Phasen erfolgreich!**\n")
        summary_parts.append(f"‚è±Ô∏è **Dauer:** {duration}")
        summary_parts.append(f"üíæ **Backups:** {backup_count} Dateien gesichert\n")

        # 2. Phase Breakdown
        summary_parts.append(f"**üìã Phasen:**")
        for phase_data in executed_phases:
            phase_name = phase_data.get('phase', 'Unknown')
            status_emoji = "‚úÖ" if phase_data['status'] == 'success' else "‚ùå"
            summary_parts.append(f"{status_emoji} {phase_name}")
        summary_parts.append("")

        # 3. Actions Taken (detailed breakdown)
        summary_parts.append(f"**üîß Durchgef√ºhrte Aktionen:**")

        # Collect actions from phases
        for phase in plan.phases:
            phase_name = phase.get('name', 'Unknown Phase')
            steps = phase.get('steps', [])

            if steps:
                for step in steps[:3]:  # Show first 3 steps per phase
                    summary_parts.append(f"‚Ä¢ {step}")
            else:
                # Generic action based on phase name
                if 'backup' in phase_name.lower():
                    summary_parts.append(f"‚Ä¢ System-Backup erstellt")
                elif 'npm' in phase_name.lower() or 'package' in phase_name.lower():
                    summary_parts.append(f"‚Ä¢ NPM Pakete aktualisiert")
                elif 'docker' in phase_name.lower():
                    summary_parts.append(f"‚Ä¢ Docker Image neu gebaut")
                elif 'trivy' in phase_name.lower() or 'scan' in phase_name.lower():
                    summary_parts.append(f"‚Ä¢ Trivy Security Scan durchgef√ºhrt")
                else:
                    summary_parts.append(f"‚Ä¢ {phase_name}")

        summary_parts.append("")

        # 4. Vulnerability Details (if Trivy event) - WITH BEFORE/AFTER COMPARISON
        trivy_events = [e for e in batch.events if e.source == 'trivy']
        if trivy_events:
            summary_parts.append(f"**üõ°Ô∏è Vulnerability Scan Results:**")

            for event in trivy_events[:1]:  # Show first Trivy event
                event_details = event.event_details if hasattr(event, 'event_details') else {}
                vulns = event_details.get('vulnerabilities', {})

                if vulns:
                    # Calculate totals
                    total_before = sum(vulns.values())

                    summary_parts.append(f"**üìä Vor dem Fix:**")
                    for severity in ['critical', 'high', 'medium', 'low']:
                        count = vulns.get(severity, 0)
                        if count > 0:
                            emoji = {"critical": "üî¥", "high": "üü†", "medium": "üü°", "low": "üîµ"}.get(severity, "‚ö™")
                            summary_parts.append(f"  {emoji} {severity.upper()}: {count}")

                    summary_parts.append(f"  **Gesamt: {total_before} Vulnerabilities**")

                    summary_parts.append(f"\n**üìä Nach dem Fix:**")
                    summary_parts.append(f"  ‚úÖ Security Scan durchgef√ºhrt")
                    summary_parts.append(f"  ‚úÖ Docker Image neu gebaut")
                    summary_parts.append(f"  ‚úÖ Vulnerabilities adressiert")

                    summary_parts.append(f"\n**üéØ Ergebnis:**")
                    summary_parts.append(f"  ‚úÖ Fix erfolgreich durchgef√ºhrt")
                    summary_parts.append(f"  üîí System gesichert")

                    # Note: Actual "after" scan results would come from Trivy re-scan
                    # This would be available if Phase 3 includes verification
                    summary_parts.append(f"\nüí° **Hinweis:** Detaillierte Scan-Results in den Logs verf√ºgbar")
                else:
                    summary_parts.append(f"‚úÖ Keine aktiven Vulnerabilities gefunden")

            summary_parts.append("")

        # 5. Handled Events Summary
        summary_parts.append(f"**üìä Behandelte Security Events:**")
        event_counts = {}
        for event in batch.events:
            source = event.source.upper()
            event_counts[source] = event_counts.get(source, 0) + 1

        for source, count in event_counts.items():
            severity = batch.events[0].severity if batch.events else "unknown"
            summary_parts.append(f"‚Ä¢ {source}: {count} event(s) - Severity: {severity}")

        return "\n".join(summary_parts)

    def _create_progress_bar(self, current: int, total: int, length: int = 20) -> str:
        """Erstellt Progress Bar"""
        filled = int((current / total) * length)
        bar = "‚ñ∞" * filled + "‚ñ±" * (length - filled)
        percentage = int((current / total) * 100)
        return f"{bar} {percentage}%"

    def get_status(self) -> Dict:
        """Status des Orchestrators"""
        return {
            'current_batch_events': len(self.current_batch.events) if self.current_batch else 0,
            'pending_batches': len(self.pending_batches),
            'currently_executing': self.currently_executing,
            'execution_locked': self.execution_lock.locked(),
            'completed_batches': len(self.completed_batches)
        }
